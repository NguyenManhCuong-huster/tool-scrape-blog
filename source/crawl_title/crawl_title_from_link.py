from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import time
from bs4 import BeautifulSoup
from docx import Document

from bs4 import BeautifulSoup
from docx import Document

def save_html_code_to_word(soup, filename="html_code.docx"):
    # Táº¡o file Word
    doc = Document()
    doc.add_heading("HTML Code Dump", level=1)

    # Láº¥y nguyÃªn mÃ£ HTML (á»Ÿ dáº¡ng string)
    html_code = soup.prettify()

    # Chia theo dÃ²ng Ä‘á»ƒ trÃ¡nh lá»—i giá»›i háº¡n Ä‘á»™ dÃ i Ä‘oáº¡n vÄƒn
    for line in html_code.splitlines():
        doc.add_paragraph(line)

    # LÆ°u file
    doc.save(filename)
    print(f"âœ… ÄÃ£ lÆ°u mÃ£ HTML vÃ o file: {filename}")

def get_soup(url, driver):
    try:
        if(driver == None): return []
        
        # KhÃ´ng má»Ÿ liÃªn tá»¥c, trÃ¡nh google ban
        time.sleep(5)
        
        # Má»Ÿ trang web
        driver.get(url)

        # Chá» 3 giÃ¢y Ä‘á»ƒ trang táº£i xong (hoáº·c dÃ¹ng WebDriverWait nÃ¢ng cao)
        time.sleep(10)

        # Láº¥y HTML
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # In thá»­ 1000 kÃ½ tá»± Ä‘áº§u tiÃªn
        #print(soup.prettify()[:1000])

        driver.quit()

        return soup

    except Exception as e:
        print(f"Lá»—i: {e}")
        return []

# Äá»™ Æ°u tiÃªn nguá»“n (nhá» hÆ¡n = Æ°u tiÃªn cao hÆ¡n)
SOURCE_PRIORITY = {
    'title': 1,
    'data-original-title': 2,
    'aria-label': 3,
    'text': 4,
    'alt': 5
}

SAFE_EXTENSIONS = ['.html', '.htm', '.php', '.asp', '.aspx', '.jsp', '.cgi', '.do', '.action']


def extract_link_info_form_url(url, driver):
    soup = get_soup(url, driver) #Láº¥y HTML cá»§a web
    if(soup == []): return []

    #save_html_code_to_word(soup, "full_html_code.docx")

    # Láº¥y domain gá»‘c tá»« url Ä‘áº§u vÃ o
    base_domain = urlparse(url).netloc.lower()


    if not soup.body:
        return []

    link_map = {}

    for a in soup.body.find_all("a", href=True):
        href = a['href'].strip()

        # âœ… PhÃ¢n tÃ­ch URL
        parsed = urlparse(href)

        """
        # âŒ Bá» náº¿u cÃ³ query (?abc=xyz) hoáº·c anchor (#abc)
        if parsed.query or parsed.fragment:
            continue
        
        # ğŸ§¹ LÃ m sáº¡ch URL: bá» anchor, query
        href = parsed._replace(query="", fragment="").geturl()
        """

        # Bá» link cÃ³ pháº§n má»Ÿ rá»™ng
        parsed = urlparse(href)
        last_part = parsed.path.rstrip('/').split('/')[-1]
        if '.' in last_part and not any(last_part.endswith(ext) for ext in SAFE_EXTENSIONS):
            continue  # bá» link cÃ³ Ä‘uÃ´i khÃ´ng há»£p lá»‡

        # âœ… Kiá»ƒm tra domain: chá»‰ giá»¯ náº¿u cÃ¹ng domain hoáº·c lÃ  link tÆ°Æ¡ng Ä‘á»‘i
        if parsed.netloc and parsed.netloc.lower() != base_domain:
            continue  # bá» náº¿u lÃ  link ngoÃ i

        # Khá»Ÿi táº¡o entry náº¿u chÆ°a cÃ³
        if href not in link_map:
            link_map[href] = {
                'has_img': False,
                'text': None,
                'source': None
            }

        entry = link_map[href]

        # âœ… Kiá»ƒm tra áº£nh
        if a.find("img"):
            entry['has_img'] = True

        # âœ… Kiá»ƒm tra cÃ¡c loáº¡i ná»™i dung theo Ä‘á»™ Æ°u tiÃªn
        for src in ['title', 'data-original-title', 'aria-label']:
            val = a.get(src)
            if val and val.strip():
                # Náº¿u chÆ°a cÃ³ text hoáº·c Ä‘á»™ Æ°u tiÃªn cao hÆ¡n â†’ cáº­p nháº­t
                if (entry['text'] is None or
                    SOURCE_PRIORITY[src] < SOURCE_PRIORITY.get(entry['source'], 999)):
                    entry['text'] = val.strip()
                    entry['source'] = src
                break  # Ä‘Ã£ láº¥y 1 loáº¡i lÃ  Ä‘á»§

        # âœ… Náº¿u chÆ°a cÃ³ text, thá»­ láº¥y text hiá»ƒn thá»‹
        if entry['text'] is None or SOURCE_PRIORITY.get(entry['source'], 999) > SOURCE_PRIORITY['text']:
            text = a.get_text(" ", strip=True)
            if text:
                entry['text'] = text
                entry['source'] = 'text'

        # âœ… Náº¿u váº«n chÆ°a cÃ³, thá»­ láº¥y alt tá»« áº£nh
        if entry['text'] is None or SOURCE_PRIORITY.get(entry['source'], 999) > SOURCE_PRIORITY['alt']:
            img = a.find('img')
            if img and img.get('alt') and img.get('alt').strip():
                entry['text'] = img.get('alt').strip()
                entry['source'] = 'alt'

    if not link_map:
        print("KhÃ´ng láº¥y Ä‘Æ°á»£c link tá»« url")
        return []

    # Chá»‰ láº¥y nhá»¯ng link cÃ³ áº£nh vÃ  text
    result = []
    for href, data in link_map.items():
        if data['has_img'] and data['text']  and data['text'].count(" ") > 3:
            result.append({
                'link': href,
                'title': data['text']
            })

    """
    print(result)
    """
    for item in result:
        print(f"ğŸ”— Link: {item['link']}")
        print(f"ğŸ“ Text: {item['title']}\n")
    
    return result

if __name__ == "__main__":
    import selenium_driver
    url = "https://careerviet.vn/vi/talentcommunity"
    driver = selenium_driver.get_chrome_driver()
    extract_link_info_form_url(url, driver)
    driver.quit()